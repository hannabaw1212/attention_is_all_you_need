{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:56:14.745559Z",
     "start_time": "2024-07-24T15:56:12.129846Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:12:53.950786Z",
     "start_time": "2024-07-24T16:12:53.947419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, vocab_size, **kwargs):\n",
    "        \"\"\"\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param vocab_size: The size of the vocabulary, defining the number of unique tokens.\n",
    "        :param kwargs: pass \n",
    "        \"\"\"\n",
    "        super(InputEmbeddings, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings= tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.embeddings(x) * np.sqrt(self.d_model)\n",
    "    \n",
    "    "
   ],
   "id": "ab8e8fbdade9f53c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:23:57.775589Z",
     "start_time": "2024-07-24T16:23:57.753483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the embedding dimensions and vocabulary size.\n",
    "d_model = 128  # Dimension of each embedding vector\n",
    "vocab_size = 1000  # Total number of unique tokens\n",
    "\n",
    "# Create an instance of the InputEmbeddings class.\n",
    "embedding_layer = InputEmbeddings(d_model=d_model, vocab_size=vocab_size)\n",
    "\n",
    "# Generate some demo input: a batch of 3 sequences, each of length 10, with random token indices.\n",
    "demo_input = tf.random.uniform((3, 10), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "\n",
    "# Get the embeddings for the demo input.\n",
    "embeddings = embedding_layer(demo_input)\n",
    "\n",
    "# Print the shape of the output embeddings.\n",
    "print(\"Shape of output embeddings:\", embeddings.shape)\n",
    "# Output the embeddings to visually inspect them (optional, as it can be large).\n",
    "print(\"Output embeddings:\", embeddings.numpy())\n",
    "print(\"Output embeddings shape:\", embeddings.shape)"
   ],
   "id": "73ae67bdacc56f38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output embeddings: (3, 10, 128)\n",
      "Output embeddings: [[[ 0.5146707   0.1895307  -0.29300112 ... -0.10862558 -0.22504583\n",
      "    0.16266403]\n",
      "  [ 0.06925477 -0.45865697  0.22549672 ...  0.5333863   0.05190561\n",
      "   -0.26408648]\n",
      "  [ 0.42520645  0.55514103 -0.13055247 ...  0.01275844  0.10262209\n",
      "   -0.441018  ]\n",
      "  ...\n",
      "  [-0.09101738 -0.29055497 -0.01321953 ... -0.50177944 -0.01079204\n",
      "   -0.13035151]\n",
      "  [ 0.35089007 -0.11844276 -0.14385305 ...  0.483865   -0.29106033\n",
      "   -0.41445133]\n",
      "  [ 0.17892529  0.37451997  0.5143618  ... -0.012647   -0.25266746\n",
      "    0.08366238]]\n",
      "\n",
      " [[-0.55149186 -0.01530908  0.40367183 ... -0.29592443  0.3236189\n",
      "    0.32945496]\n",
      "  [ 0.5608614   0.03319309 -0.33201817 ...  0.45180464  0.2990552\n",
      "   -0.43874127]\n",
      "  [-0.19392823 -0.36530343 -0.0024843  ... -0.5620586  -0.5649589\n",
      "    0.48271498]\n",
      "  ...\n",
      "  [ 0.27567154 -0.14037745 -0.0081402  ... -0.48876196 -0.1506169\n",
      "   -0.14031284]\n",
      "  [ 0.40844035 -0.550867   -0.23408803 ...  0.24759406 -0.49529007\n",
      "    0.40793145]\n",
      "  [-0.54291886  0.45541257 -0.11812177 ...  0.4943631  -0.20765676\n",
      "    0.24132235]]\n",
      "\n",
      " [[-0.06780572  0.3743389   0.54905003 ... -0.3990759   0.07883766\n",
      "    0.10816487]\n",
      "  [ 0.10547408  0.3512259  -0.20737733 ... -0.10957506  0.49055865\n",
      "   -0.23736916]\n",
      "  [ 0.39572623  0.17341046  0.21060641 ...  0.44009203 -0.14835016\n",
      "   -0.5654795 ]\n",
      "  ...\n",
      "  [ 0.41803303 -0.16124305  0.2080529  ...  0.1034977  -0.3575473\n",
      "   -0.01051298]\n",
      "  [ 0.47911403  0.0945771  -0.11091027 ... -0.08522809 -0.5262396\n",
      "   -0.14373988]\n",
      "  [ 0.0869304   0.5126898  -0.04245637 ...  0.37495896  0.42861158\n",
      "    0.51951677]]]\n",
      "Output embeddings shape: (3, 10, 128)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:43:02.407060Z",
     "start_time": "2024-07-24T16:43:02.403968Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings.numpy()[0]",
   "id": "a1d748b9d2a67009",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5146707 ,  0.1895307 , -0.29300112, ..., -0.10862558,\n",
       "        -0.22504583,  0.16266403],\n",
       "       [ 0.06925477, -0.45865697,  0.22549672, ...,  0.5333863 ,\n",
       "         0.05190561, -0.26408648],\n",
       "       [ 0.42520645,  0.55514103, -0.13055247, ...,  0.01275844,\n",
       "         0.10262209, -0.441018  ],\n",
       "       ...,\n",
       "       [-0.09101738, -0.29055497, -0.01321953, ..., -0.50177944,\n",
       "        -0.01079204, -0.13035151],\n",
       "       [ 0.35089007, -0.11844276, -0.14385305, ...,  0.483865  ,\n",
       "        -0.29106033, -0.41445133],\n",
       "       [ 0.17892529,  0.37451997,  0.5143618 , ..., -0.012647  ,\n",
       "        -0.25266746,  0.08366238]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding\n",
    "\n",
    "The main purpose of positional encodings is to give the model some information about the order of words or the relative positions of words within a sequence. This is crucial for tasks involving language where the meaning of a sentence can change dramatically based on the order of words (e.g., \"I like dogs more than cats\" vs \"I like cats more than dogs\").\n",
    "\n"
   ],
   "id": "78927f1900b3bf01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, max_len:int, dropout:float,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding layer.\n",
    "\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param max_len: The maximum number of positions for which embeddings will be created.\n",
    "        :param droupout: The dropout rate to apply to the output of this layer.\n",
    "        :param kwargs: pass\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.dropout = tf.Droupout(rate=dropout)\n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "        \n",
    "    def _get_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Generates the positional encodings using sinusoidal patterns.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: A tensor containing positional encodings of shape (1, max_len, d_model).\n",
    "        \"\"\"\n",
    "        positions = np.arange(self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))  # Shape (d_model/2,)\n",
    "        \n",
    "        "
   ],
   "id": "da6da8613c516afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T17:00:33.892589Z",
     "start_time": "2024-07-24T17:00:33.876741Z"
    }
   },
   "cell_type": "code",
   "source": "np.arange(5)[:...]",
   "id": "3c74fbc77dfb8fb7",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "931745d4c44ec82d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T17:06:53.394434Z",
     "start_time": "2024-07-24T17:06:53.390573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) /d_model))  # Shape (d_model/2,)\n",
    "d_model = 512  # Dimensionality of the embedding\n",
    "max_len = 10  # Maximum length of the sequence\n",
    "dropout_rate = 0.1  # Dropout rate\n"
   ],
   "id": "fa98e7af3e9acf9a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T17:06:57.005974Z",
     "start_time": "2024-07-24T17:06:57.002474Z"
    }
   },
   "cell_type": "code",
   "source": "div_term",
   "id": "bcb7b7803490234a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 8.65964323e-01, 7.49894209e-01, 6.49381632e-01,\n",
       "       5.62341325e-01, 4.86967525e-01, 4.21696503e-01, 3.65174127e-01,\n",
       "       3.16227766e-01, 2.73841963e-01, 2.37137371e-01, 2.05352503e-01,\n",
       "       1.77827941e-01, 1.53992653e-01, 1.33352143e-01, 1.15478198e-01,\n",
       "       1.00000000e-01, 8.65964323e-02, 7.49894209e-02, 6.49381632e-02,\n",
       "       5.62341325e-02, 4.86967525e-02, 4.21696503e-02, 3.65174127e-02,\n",
       "       3.16227766e-02, 2.73841963e-02, 2.37137371e-02, 2.05352503e-02,\n",
       "       1.77827941e-02, 1.53992653e-02, 1.33352143e-02, 1.15478198e-02,\n",
       "       1.00000000e-02, 8.65964323e-03, 7.49894209e-03, 6.49381632e-03,\n",
       "       5.62341325e-03, 4.86967525e-03, 4.21696503e-03, 3.65174127e-03,\n",
       "       3.16227766e-03, 2.73841963e-03, 2.37137371e-03, 2.05352503e-03,\n",
       "       1.77827941e-03, 1.53992653e-03, 1.33352143e-03, 1.15478198e-03,\n",
       "       1.00000000e-03, 8.65964323e-04, 7.49894209e-04, 6.49381632e-04,\n",
       "       5.62341325e-04, 4.86967525e-04, 4.21696503e-04, 3.65174127e-04,\n",
       "       3.16227766e-04, 2.73841963e-04, 2.37137371e-04, 2.05352503e-04,\n",
       "       1.77827941e-04, 1.53992653e-04, 1.33352143e-04, 1.15478198e-04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
