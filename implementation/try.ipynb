{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.075365Z",
     "start_time": "2024-07-28T18:14:02.995978Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.091592Z",
     "start_time": "2024-07-28T18:14:05.076388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, vocab_size, **kwargs):\n",
    "        \"\"\"\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param vocab_size: The size of the vocabulary, defining the number of unique tokens.\n",
    "        :param kwargs: pass \n",
    "        \"\"\"\n",
    "        super(InputEmbeddings, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings= tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.embeddings(x) * np.sqrt(self.d_model)\n",
    "    \n",
    "    "
   ],
   "id": "ab8e8fbdade9f53c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.139479Z",
     "start_time": "2024-07-28T18:14:05.091592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here is another sentence.\",\n",
    "    \"Yet another example sentence.\",\n",
    "    \"This is the final test sentence.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Get the maximum sequence length for padding\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
    "\n",
    "# Define the model parameters\n",
    "d_model = 16\n",
    "\n",
    "# Create an instance of the InputEmbeddings layer\n",
    "embedding_layer = InputEmbeddings(d_model, vocab_size)\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "embeddings_output = embedding_layer(padded_sequences)\n",
    "\n",
    "# Print the output embeddings\n",
    "print(embeddings_output)\n",
    "print(embeddings_output.shape)\n"
   ],
   "id": "73ae67bdacc56f38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.11257984 -0.08856998  0.0926858  -0.03721432 -0.07346258\n",
      "   -0.06355219  0.0730051  -0.06409092  0.12110634 -0.10530062\n",
      "    0.06210451  0.18152653  0.01275554 -0.17367946  0.15521078\n",
      "    0.16727619]\n",
      "  [-0.01022848 -0.17847104  0.07384934 -0.03298935  0.10027124\n",
      "    0.07477613 -0.18374644  0.16860901 -0.19943972  0.0578679\n",
      "    0.02327628 -0.03379402  0.14394106 -0.06908861 -0.07943449\n",
      "   -0.19171272]\n",
      "  [-0.07945347 -0.12175112 -0.12724987 -0.02764472  0.17385592\n",
      "    0.09522177 -0.00285444  0.14376272  0.18644036 -0.05858894\n",
      "   -0.12442055 -0.13338466  0.18802051  0.09808816 -0.1432445\n",
      "   -0.1927384 ]\n",
      "  [ 0.12398507  0.0307304  -0.03567472  0.19479193 -0.07170348\n",
      "    0.00606112  0.18673457 -0.14331993  0.18821092 -0.06485137\n",
      "    0.0019834  -0.0861259   0.06197052 -0.13573009 -0.1991003\n",
      "   -0.04486199]\n",
      "  [-0.07074657 -0.14192633  0.08297123  0.00757518 -0.16485281\n",
      "    0.01533937  0.00699195  0.08012719  0.10672225  0.19394918\n",
      "    0.02697448 -0.08011355  0.12759005 -0.1704752   0.00491376\n",
      "   -0.00517789]\n",
      "  [ 0.01768956 -0.06399384  0.09067298 -0.05285659 -0.12258973\n",
      "    0.18414827 -0.09331622 -0.19202457  0.15373422 -0.06141344\n",
      "    0.105958    0.19406115  0.15815903  0.11689945 -0.05324546\n",
      "    0.04914817]]\n",
      "\n",
      " [[ 0.17092209  0.0838726   0.13291122 -0.05575629  0.07602562\n",
      "   -0.13860922 -0.07934022 -0.00215606  0.11716332 -0.01895913\n",
      "    0.01192932  0.11694466  0.11597972  0.16282155  0.06930022\n",
      "   -0.17197518]\n",
      "  [-0.01022848 -0.17847104  0.07384934 -0.03298935  0.10027124\n",
      "    0.07477613 -0.18374644  0.16860901 -0.19943972  0.0578679\n",
      "    0.02327628 -0.03379402  0.14394106 -0.06908861 -0.07943449\n",
      "   -0.19171272]\n",
      "  [-0.19320117 -0.07038426 -0.15023609  0.003059    0.02845207\n",
      "   -0.0493401  -0.19783497 -0.18105797 -0.07127552 -0.07590132\n",
      "    0.0969211   0.05790724  0.12115265 -0.11324115  0.15069576\n",
      "   -0.18165755]\n",
      "  [-0.07074657 -0.14192633  0.08297123  0.00757518 -0.16485281\n",
      "    0.01533937  0.00699195  0.08012719  0.10672225  0.19394918\n",
      "    0.02697448 -0.08011355  0.12759005 -0.1704752   0.00491376\n",
      "   -0.00517789]\n",
      "  [ 0.01768956 -0.06399384  0.09067298 -0.05285659 -0.12258973\n",
      "    0.18414827 -0.09331622 -0.19202457  0.15373422 -0.06141344\n",
      "    0.105958    0.19406115  0.15815903  0.11689945 -0.05324546\n",
      "    0.04914817]\n",
      "  [ 0.01768956 -0.06399384  0.09067298 -0.05285659 -0.12258973\n",
      "    0.18414827 -0.09331622 -0.19202457  0.15373422 -0.06141344\n",
      "    0.105958    0.19406115  0.15815903  0.11689945 -0.05324546\n",
      "    0.04914817]]\n",
      "\n",
      " [[-0.04610315 -0.16294241  0.0410088  -0.15527043  0.09553523\n",
      "    0.10122786 -0.10083022 -0.1766419   0.02678891  0.02889538\n",
      "   -0.08438426 -0.17674199 -0.11034956  0.19463731 -0.05756864\n",
      "   -0.12621075]\n",
      "  [-0.19320117 -0.07038426 -0.15023609  0.003059    0.02845207\n",
      "   -0.0493401  -0.19783497 -0.18105797 -0.07127552 -0.07590132\n",
      "    0.0969211   0.05790724  0.12115265 -0.11324115  0.15069576\n",
      "   -0.18165755]\n",
      "  [-0.05872282 -0.07547002  0.01381198 -0.08349266  0.1917441\n",
      "    0.08953072  0.06231995  0.05682997 -0.05983314 -0.00232725\n",
      "    0.17615987  0.17225222 -0.14686003 -0.07644468 -0.04356651\n",
      "   -0.03033409]\n",
      "  [-0.07074657 -0.14192633  0.08297123  0.00757518 -0.16485281\n",
      "    0.01533937  0.00699195  0.08012719  0.10672225  0.19394918\n",
      "    0.02697448 -0.08011355  0.12759005 -0.1704752   0.00491376\n",
      "   -0.00517789]\n",
      "  [ 0.01768956 -0.06399384  0.09067298 -0.05285659 -0.12258973\n",
      "    0.18414827 -0.09331622 -0.19202457  0.15373422 -0.06141344\n",
      "    0.105958    0.19406115  0.15815903  0.11689945 -0.05324546\n",
      "    0.04914817]\n",
      "  [ 0.01768956 -0.06399384  0.09067298 -0.05285659 -0.12258973\n",
      "    0.18414827 -0.09331622 -0.19202457  0.15373422 -0.06141344\n",
      "    0.105958    0.19406115  0.15815903  0.11689945 -0.05324546\n",
      "    0.04914817]]\n",
      "\n",
      " [[ 0.11257984 -0.08856998  0.0926858  -0.03721432 -0.07346258\n",
      "   -0.06355219  0.0730051  -0.06409092  0.12110634 -0.10530062\n",
      "    0.06210451  0.18152653  0.01275554 -0.17367946  0.15521078\n",
      "    0.16727619]\n",
      "  [-0.01022848 -0.17847104  0.07384934 -0.03298935  0.10027124\n",
      "    0.07477613 -0.18374644  0.16860901 -0.19943972  0.0578679\n",
      "    0.02327628 -0.03379402  0.14394106 -0.06908861 -0.07943449\n",
      "   -0.19171272]\n",
      "  [-0.07945347 -0.12175112 -0.12724987 -0.02764472  0.17385592\n",
      "    0.09522177 -0.00285444  0.14376272  0.18644036 -0.05858894\n",
      "   -0.12442055 -0.13338466  0.18802051  0.09808816 -0.1432445\n",
      "   -0.1927384 ]\n",
      "  [-0.1932559   0.05890475  0.11664043  0.19705753  0.00577006\n",
      "    0.14592387  0.14280616 -0.19319443 -0.16674657 -0.036341\n",
      "   -0.06197533 -0.11664452  0.19313617  0.08606867 -0.03555313\n",
      "   -0.08131285]\n",
      "  [-0.168504   -0.00063868  0.09503783  0.15227352 -0.03206292\n",
      "    0.09074856 -0.11099964 -0.06709556 -0.16854724  0.05072452\n",
      "   -0.0015308   0.09300338  0.10250451  0.1488411   0.1703227\n",
      "   -0.19542523]\n",
      "  [-0.07074657 -0.14192633  0.08297123  0.00757518 -0.16485281\n",
      "    0.01533937  0.00699195  0.08012719  0.10672225  0.19394918\n",
      "    0.02697448 -0.08011355  0.12759005 -0.1704752   0.00491376\n",
      "   -0.00517789]]], shape=(4, 6, 16), dtype=float32)\n",
      "(4, 6, 16)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The shape `(4, 6, 16)` can be interpreted as follows:\n",
    "\n",
    "1. **4**: This dimension represents the number of samples or batches. In this context, it corresponds to the 4 sentences being processed.\n",
    "2. **6**: This dimension represents the sequence length, which is the maximum number of tokens (words) in the padded sentences. In this case, each sentence has been padded to a length of 6 tokens.\n",
    "3. **16**: This dimension represents the size of each embedding vector, denoted by `d_model` in the `InputEmbeddings` class. Each token in the input sequences is mapped to a 16-dimensional vector.\n",
    "\n",
    "So, the tensor with shape `(4, 6, 16)` represents the embeddings of 4 sentences, each with a sequence length of 6 tokens, where each token is represented by a 16-dimensional embedding vector."
   ],
   "id": "f0d4dcce1b1894b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.486366Z",
     "start_time": "2024-07-28T18:14:05.140495Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings.numpy()[0]",
   "id": "a1d748b9d2a67009",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43membeddings\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding\n",
    "\n",
    "The main purpose of positional encodings is to give the model some information about the order of words or the relative positions of words within a sequence. This is crucial for tasks involving language where the meaning of a sentence can change dramatically based on the order of words (e.g., \"I like dogs more than cats\" vs \"I like cats more than dogs\").\n",
    "\n"
   ],
   "id": "78927f1900b3bf01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.487371Z",
     "start_time": "2024-07-28T18:14:05.487371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, max_len:int, dropout:float,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding layer.\n",
    "\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param max_len: The maximum number of positions for which embeddings will be created.\n",
    "        :param droupout: The dropout rate to apply to the output of this layer.\n",
    "        :param kwargs: pass\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout)\n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "        \n",
    "    def _get_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Generates the positional encodings using sinusoidal patterns.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: A tensor containing positional encodings of shape (1, max_len, d_model).\n",
    "        \"\"\"\n",
    "        positions = np.arange(self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))  # Shape (d_model/2,)\n",
    "        \n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        pe[:, 0::2] = np.sin(positions * div_term)\n",
    "        pe[:, 1::2] = np.cos(positions * div_term)\n",
    "        \n",
    "        pe = pe[np.newaxis, ...]  # Add a new batch dimension (1, max_len, d_model)\n",
    "        return tf.cast(pe, tf.float32)\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        assert seq_len <= self.max_len, \"Input sequence length exceeds the maximum length\"\n",
    "        \n",
    "        x = x + self.positional_encoding[:, :seq_len]\n",
    "        return self.dropout(x)\n",
    "    \n"
   ],
   "id": "da6da8613c516afc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:05.488371Z",
     "start_time": "2024-07-28T18:14:05.487371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters for the model\n",
    "d_model = 64  # Dimensionality of the embeddings\n",
    "max_len = 10  # Assume max sentence length\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the PositionalEncoding layer\n",
    "pos_encoding_layer = PositionalEncoding(d_model, max_len, dropout_rate)\n",
    "\n",
    "# Generate dummy embeddings for 4 sentences, each with varying lengths\n",
    "dummy_embeddings = tf.random.normal((4, max_len, d_model))  # Shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "# Apply the PositionalEncoding layer\n",
    "encoded_embeddings = pos_encoding_layer(dummy_embeddings)\n",
    "\n",
    "# Print the shape and some values to verify\n",
    "print(\"Shape of encoded embeddings:\", encoded_embeddings.shape)\n",
    "print(\"Sample values from encoded embeddings:\", encoded_embeddings[0, :3, :5])  # Print first 3 positions of the first sentence\n"
   ],
   "id": "3c74fbc77dfb8fb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Add & Norm Layer\n",
    "\n",
    "1. **Residual Connection (Add)**: This step adds the original input of the sub-layer to the output of the sub-layer (like self-attention or feed-forward network), creating a shortcut connection.\n",
    "2. **Layer Normalization (Norm)**: This step normalizes the sum to stabilize and accelerate training by normalizing the output across the features.\n"
   ],
   "id": "85f0b616b55eb00d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AddAndNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the AddAndNorm layer.\n",
    "\n",
    "        :param epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        :param kwargs: Additional arguments for the layer.\n",
    "        \"\"\"\n",
    "        super(AddAndNorm, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "        \n",
    "    def call(self, x, sub_layer_output):\n",
    "        return self.layer_norm(x + sub_layer_output)\n",
    "        "
   ],
   "id": "931745d4c44ec82d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = tf.random.uniform((4, 6, 16))  # Example input tensor\n",
    "sub_layer_output = tf.random.uniform((4, 6, 16))  # Example sub-layer output\n",
    "\n",
    "add_and_norm_layer = AddAndNorm()\n",
    "output = add_and_norm_layer(x, sub_layer_output)\n",
    "print(output)\n"
   ],
   "id": "e02e9d32abb7be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "The \"Feed Forward\" layer in transformer architectures refers to a fully connected feed-forward network applied to each position separately and identically. Typically, it consists of two linear transformations with a ReLU activation in between. The purpose of this layer is to introduce non-linearity and expand the dimensionality of the embeddings before reducing it back to the original size.\n",
    "\n"
   ],
   "id": "f16ae257e1d49c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout:float, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward layer.\n",
    "\n",
    "        :param d_model: The size of the input and output embeddings.\n",
    "        :param d_ff: The hidden layer size of the feed-forward network.\n",
    "        :param dropout_rate: The dropout rate to apply to the output of the feed-forward network.\n",
    "        \"\"\"    \n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the FeedForward layer.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor after applying the feed-forward network and dropout.\n",
    "        \"\"\"\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    "
   ],
   "id": "9034d853d1e841f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentences = [\n",
    "    \"The quick brown fox jumps.\",\n",
    "    \"Over the lazy dog.\",\n",
    "    \"And runs away swiftly.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the sequences to the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "\n",
    "# Define the embedding size (d_model)\n",
    "d_model = 16\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "\n",
    "embeddings_output = embedding_layer(padded_sequences)\n",
    "embeddings_output.shape"
   ],
   "id": "a316dc7985acd242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "d_ff = 64\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create an instance of the FeedForward layer\n",
    "feed_forward_layer = FeedForward(d_model, d_ff, dropout_rate)\n",
    "\n",
    "# Apply the FeedForward layer to the embeddings\n",
    "feed_forward_output = feed_forward_layer(embeddings_output)\n",
    "\n",
    "print(feed_forward_output)\n"
   ],
   "id": "c4af9b3e1a861a9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "efd9c536e19874c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_tensor = tf.random.uniform((4, 6, d_model))  # Shape (batch_size, sequence_length, d_model)\n",
    "input_tensor[0].shape"
   ],
   "id": "9d9d8b9e8d9824f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "A = np.random.randn(3, 5, 16)\n",
    "W = np.random.randn(16, 4)\n",
    "\n",
    "tf.matmul(A, W)"
   ],
   "id": "5177f9cf1fdfaf66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention is a key component of the transformer architecture, enabling the model to focus on different parts of the input sequence to capture contextual relationships. Here is a detailed explanation with the relevant equations and an algorithm in words.\n",
    "\n",
    "\n",
    "### Notations\n",
    "- $\\mathbf{X} $: Input matrix of shape $(\\text{batch\\_size}, \\text{seq\\_len}, \\text{d\\_model}$)\n",
    "- $ \\mathbf{Q}$, $ \\mathbf{K} $, $ \\mathbf{V} $: Query, Key, and Value matrices\n",
    "- $ \\mathbf{W}_Q $, $ \\mathbf{W}_K $, $ \\mathbf{W}_V $: Weight matrices for transforming input into queries, keys, and values\n",
    "- $ \\text{d\\_model} $: Dimension of the model (embedding size)\n",
    "- $ \\text{num\\_heads} $: Number of attention heads\n",
    "- $\\text{depth} = \\frac{\\text{d\\_model}}{\\text{num\\_heads}} $: Dimension of each head\n",
    "- $ \\mathbf{W}_O $: Final output weight matrix\n",
    "- $ \\mathbf{O} $: Final output after multi-head attention\n",
    "\n",
    "### 1. Linear Projections\n",
    "The input $\\mathbf{X}$ is linearly projected into three different matrices: queries ($\\mathbf{Q}$), keys ($\\mathbf{K}$), and values ($\\mathbf{V}$). These are obtained by multiplying $\\mathbf{X}$ with three learned weight matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X} \\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X} \\mathbf{W}_V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{\\text{d\\_model} \\times \\text{d\\_model}} $ are weight matrices.\n",
    "\n",
    "### 2. Split into Multiple Heads\n",
    "The query, key, and value matrices are split into multiple heads to allow the model to jointly attend to information from different representation subspaces at different positions. Each head has its own set of Q, K, V matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}_i = \\mathbf{Q}[:, :, i \\times \\text{depth} : (i+1) \\times \\text{depth}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{K}_i = \\mathbf{K}[:, :, i \\times \\text{depth} : (i+1) \\times \\text{depth}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_i = \\mathbf{V}[:, :, i \\times \\text{depth} : (i+1) \\times \\text{depth}]\n",
    "$$\n",
    "\n",
    "for $ i = 0, 1, \\ldots, \\text{num\\_heads} - 1 $.\n",
    "\n",
    "### 3. Scaled Dot-Product Attention\n",
    "For each head, the attention scores are computed using the scaled dot-product attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{\\text{depth}}}\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The scaling factor $sqrt{\\text{depth}}$ prevents the dot products from growing too large.\n",
    "\n",
    "### 4. Concatenation of Heads\n",
    "The outputs of each attention head are concatenated:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_{\\text{num\\_heads}})\n",
    "$$\n",
    "\n",
    "### 5. Final Linear Transformation\n",
    "The concatenated output $\\mathbf{O}$ is then linearly transformed with a final weight matrix $\\mathbf{W}_O$:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\mathbf{O} \\mathbf{W}_O\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{W}_O \\in \\mathbb{R}^{\\text{num\\_heads} \\cdot \\text{depth} \\times \\text{d\\_model}} $.\n",
    "\n",
    "\n"
   ],
   "id": "44d0b64701b8c748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, **kwargs):\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    self.depth = d_model // num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  \n",
    "  def split_into_heads(self, x, batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result to shape (batch_size, num_heads, seq_len, depth).\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) # shape: (batch_size, seq_len_q, num_heads, depth)\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3]) # shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(v)[0]\n",
    "\n",
    "    q = self.wq(q)\n",
    "    v = self.wv(v)\n",
    "    k = self.wk(k)\n",
    "\n",
    "    q_splitted = self.split_into_heads(q, batch_size)\n",
    "    v_splitted = self.split_into_heads(v, batch_size)\n",
    "    k_splitted = self.split_into_heads(k, batch_size)\n",
    "\n",
    "    scaled_attention, attention_weights = self.scaled_dot_product_attention(q_splitted, k_splitted, v_splitted, mask)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "  \n",
    "  def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead) but it must be\n",
    "        broadcastable for addition.\n",
    "    \"\"\"\n",
    "    q_k_dot = tf.matmul(q, k, transpose_b=True) # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_q_k_dot = q_k_dot / np.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "      scaled_q_k_dot += (mask * -1e9) # Add the mask to the scaled tensor.\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_q_k_dot, axis=-1) # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights\n"
   ],
   "id": "c0a66766a4a7855b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.random.randn(3, 2, 4)\n",
    "x\n",
    "\n"
   ],
   "id": "c6bffb2bbc05b932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.reshape(3, -1, 2, 2)",
   "id": "470642703b03ba93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Encoder",
   "id": "3fb62cfe8905974a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:14:10.050494Z",
     "start_time": "2024-07-28T18:14:09.896619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from implementation.encoder.EncoderLayer import EncoderLayer\n",
    "\n",
    "dummy_input = tf.random.uniform((1, 10, 512), dtype=tf.float32)\n",
    "dummy_mask = np.zeros((1, 1, 10))  # No actual masking for simplicity\n",
    "\n",
    "# Instantiate the EncoderLayer\n",
    "encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "# Pass the dummy input through the encoder layer\n",
    "output = encoder_layer(dummy_input, dummy_mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n"
   ],
   "id": "ec24c3be6154a5c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 10, 512)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test decoder",
   "id": "8e916e2cc803e147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:27:02.594389Z",
     "start_time": "2024-07-28T18:27:02.398362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from implementation.decoder.DecoderLayer import DecoderLayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dummy_decoder_input = tf.random.uniform((1, 10, 512), dtype=tf.float32)\n",
    "dummy_encoder_output = tf.random.uniform((1, 10, 512), dtype=tf.float32)\n",
    "look_ahead_mask = np.ones((1, 1, 10))  # Simplified mask for demonstration\n",
    "padding_mask = np.zeros((1, 1, 10))  # Simplified mask for demonstration\n",
    "\n",
    "decoder_layer = DecoderLayer(d_model=512, num_heads=8, d_ff=2048)\n",
    "decoder_output, block1, block2 = decoder_layer(\n",
    "    dummy_decoder_input, dummy_encoder_output, look_ahead_mask, padding_mask\n",
    ")\n",
    "\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n"
   ],
   "id": "b39a5892bc34e8be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (1, 10, 512)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b93a3ed0f12b69d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
