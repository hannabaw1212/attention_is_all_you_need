{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:20.171791Z",
     "start_time": "2024-07-24T19:11:15.275098Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:20.194036Z",
     "start_time": "2024-07-24T19:11:20.172796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, vocab_size, **kwargs):\n",
    "        \"\"\"\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param vocab_size: The size of the vocabulary, defining the number of unique tokens.\n",
    "        :param kwargs: pass \n",
    "        \"\"\"\n",
    "        super(InputEmbeddings, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings= tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.embeddings(x) * np.sqrt(self.d_model)\n",
    "    \n",
    "    "
   ],
   "id": "ab8e8fbdade9f53c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:20.247593Z",
     "start_time": "2024-07-24T19:11:20.194036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here is another sentence.\",\n",
    "    \"Yet another example sentence.\",\n",
    "    \"This is the final test sentence.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Get the maximum sequence length for padding\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
    "\n",
    "# Define the model parameters\n",
    "d_model = 16\n",
    "\n",
    "# Create an instance of the InputEmbeddings layer\n",
    "embedding_layer = InputEmbeddings(d_model, vocab_size)\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "embeddings_output = embedding_layer(padded_sequences)\n",
    "\n",
    "# Print the output embeddings\n",
    "print(embeddings_output)\n",
    "print(embeddings_output.shape)\n"
   ],
   "id": "73ae67bdacc56f38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.13128743  0.04554224 -0.10136862  0.19960685  0.0605513\n",
      "    0.04189858 -0.03823599  0.17691298  0.03172989 -0.15908423\n",
      "   -0.06517415  0.00691295  0.07703792  0.0201828  -0.0541226\n",
      "   -0.04403678]\n",
      "  [-0.0250515  -0.01304536  0.13116775 -0.04815464  0.1858624\n",
      "   -0.06104913 -0.14646482  0.00809726  0.13263626 -0.06886563\n",
      "   -0.15089998  0.01914987  0.0643983  -0.03992887 -0.11464443\n",
      "   -0.1406134 ]\n",
      "  [ 0.16751815 -0.04126629  0.1747637  -0.05674067  0.16145505\n",
      "   -0.06902881  0.15471573  0.14071198 -0.15329476 -0.19562793\n",
      "    0.19865619 -0.09923406  0.02197394 -0.01993647  0.04349986\n",
      "    0.10667048]\n",
      "  [ 0.18482341  0.12821461  0.09862529  0.16479765  0.19569837\n",
      "   -0.1164505   0.00587115 -0.11567082  0.1956123  -0.06940946\n",
      "   -0.02196345 -0.00231476  0.10348244 -0.095115   -0.05710916\n",
      "   -0.02115765]\n",
      "  [-0.12518068 -0.00631586  0.0892501  -0.1334598  -0.19918633\n",
      "    0.04776707 -0.06120186  0.06519438 -0.05054636 -0.15564366\n",
      "   -0.19777803  0.07249309  0.13974081 -0.1400052  -0.0025198\n",
      "    0.0914904 ]\n",
      "  [-0.11621767  0.15579139  0.04861522  0.12949906  0.12537841\n",
      "   -0.02028108 -0.17337508 -0.1093215  -0.16852064  0.00592795\n",
      "   -0.0287334  -0.13214421 -0.19514842  0.15517063 -0.1557168\n",
      "   -0.11997123]]\n",
      "\n",
      " [[ 0.02682623 -0.13430062  0.01921968 -0.09616051 -0.1996241\n",
      "   -0.18137641 -0.06792846 -0.06537113  0.07772885 -0.14336419\n",
      "   -0.03498396 -0.11630435 -0.10437441  0.08436538  0.06295122\n",
      "    0.13094224]\n",
      "  [-0.0250515  -0.01304536  0.13116775 -0.04815464  0.1858624\n",
      "   -0.06104913 -0.14646482  0.00809726  0.13263626 -0.06886563\n",
      "   -0.15089998  0.01914987  0.0643983  -0.03992887 -0.11464443\n",
      "   -0.1406134 ]\n",
      "  [ 0.0448254  -0.1116302  -0.10341101 -0.1492385   0.13460623\n",
      "    0.10951214  0.18398471  0.09579448 -0.02254196  0.12048002\n",
      "    0.06928994  0.01489882  0.17220964  0.03507122  0.02294183\n",
      "    0.17020993]\n",
      "  [-0.12518068 -0.00631586  0.0892501  -0.1334598  -0.19918633\n",
      "    0.04776707 -0.06120186  0.06519438 -0.05054636 -0.15564366\n",
      "   -0.19777803  0.07249309  0.13974081 -0.1400052  -0.0025198\n",
      "    0.0914904 ]\n",
      "  [-0.11621767  0.15579139  0.04861522  0.12949906  0.12537841\n",
      "   -0.02028108 -0.17337508 -0.1093215  -0.16852064  0.00592795\n",
      "   -0.0287334  -0.13214421 -0.19514842  0.15517063 -0.1557168\n",
      "   -0.11997123]\n",
      "  [-0.11621767  0.15579139  0.04861522  0.12949906  0.12537841\n",
      "   -0.02028108 -0.17337508 -0.1093215  -0.16852064  0.00592795\n",
      "   -0.0287334  -0.13214421 -0.19514842  0.15517063 -0.1557168\n",
      "   -0.11997123]]\n",
      "\n",
      " [[ 0.13988204  0.04644914  0.08529358 -0.03125162 -0.00625439\n",
      "   -0.06884785  0.16118841 -0.09686632  0.0002555   0.0020382\n",
      "    0.06570615  0.12088861  0.06481625 -0.03902808 -0.1499311\n",
      "   -0.13068648]\n",
      "  [ 0.0448254  -0.1116302  -0.10341101 -0.1492385   0.13460623\n",
      "    0.10951214  0.18398471  0.09579448 -0.02254196  0.12048002\n",
      "    0.06928994  0.01489882  0.17220964  0.03507122  0.02294183\n",
      "    0.17020993]\n",
      "  [ 0.041959   -0.0681694  -0.0044837  -0.05133834 -0.15507683\n",
      "   -0.16091971  0.03716746 -0.17126818 -0.0364376  -0.1865286\n",
      "    0.11652155  0.00751586 -0.16837172  0.02930155 -0.1457788\n",
      "    0.1868261 ]\n",
      "  [-0.12518068 -0.00631586  0.0892501  -0.1334598  -0.19918633\n",
      "    0.04776707 -0.06120186  0.06519438 -0.05054636 -0.15564366\n",
      "   -0.19777803  0.07249309  0.13974081 -0.1400052  -0.0025198\n",
      "    0.0914904 ]\n",
      "  [-0.11621767  0.15579139  0.04861522  0.12949906  0.12537841\n",
      "   -0.02028108 -0.17337508 -0.1093215  -0.16852064  0.00592795\n",
      "   -0.0287334  -0.13214421 -0.19514842  0.15517063 -0.1557168\n",
      "   -0.11997123]\n",
      "  [-0.11621767  0.15579139  0.04861522  0.12949906  0.12537841\n",
      "   -0.02028108 -0.17337508 -0.1093215  -0.16852064  0.00592795\n",
      "   -0.0287334  -0.13214421 -0.19514842  0.15517063 -0.1557168\n",
      "   -0.11997123]]\n",
      "\n",
      " [[-0.13128743  0.04554224 -0.10136862  0.19960685  0.0605513\n",
      "    0.04189858 -0.03823599  0.17691298  0.03172989 -0.15908423\n",
      "   -0.06517415  0.00691295  0.07703792  0.0201828  -0.0541226\n",
      "   -0.04403678]\n",
      "  [-0.0250515  -0.01304536  0.13116775 -0.04815464  0.1858624\n",
      "   -0.06104913 -0.14646482  0.00809726  0.13263626 -0.06886563\n",
      "   -0.15089998  0.01914987  0.0643983  -0.03992887 -0.11464443\n",
      "   -0.1406134 ]\n",
      "  [ 0.16751815 -0.04126629  0.1747637  -0.05674067  0.16145505\n",
      "   -0.06902881  0.15471573  0.14071198 -0.15329476 -0.19562793\n",
      "    0.19865619 -0.09923406  0.02197394 -0.01993647  0.04349986\n",
      "    0.10667048]\n",
      "  [-0.12287789  0.01372179 -0.03978929 -0.13485718  0.0153347\n",
      "   -0.16214915  0.10942413  0.06876029  0.02409181  0.0900396\n",
      "    0.04714298  0.04151268  0.14050351  0.0326229   0.06076069\n",
      "    0.12684198]\n",
      "  [ 0.02787976 -0.14449897  0.05515485 -0.10660491  0.12495269\n",
      "    0.00436291  0.13635807  0.02685675  0.08903386 -0.0298862\n",
      "    0.07664905 -0.15850993  0.07555486  0.09190236 -0.19447131\n",
      "    0.1398087 ]\n",
      "  [-0.12518068 -0.00631586  0.0892501  -0.1334598  -0.19918633\n",
      "    0.04776707 -0.06120186  0.06519438 -0.05054636 -0.15564366\n",
      "   -0.19777803  0.07249309  0.13974081 -0.1400052  -0.0025198\n",
      "    0.0914904 ]]], shape=(4, 6, 16), dtype=float32)\n",
      "(4, 6, 16)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The shape `(4, 6, 16)` can be interpreted as follows:\n",
    "\n",
    "1. **4**: This dimension represents the number of samples or batches. In this context, it corresponds to the 4 sentences being processed.\n",
    "2. **6**: This dimension represents the sequence length, which is the maximum number of tokens (words) in the padded sentences. In this case, each sentence has been padded to a length of 6 tokens.\n",
    "3. **16**: This dimension represents the size of each embedding vector, denoted by `d_model` in the `InputEmbeddings` class. Each token in the input sequences is mapped to a 16-dimensional vector.\n",
    "\n",
    "So, the tensor with shape `(4, 6, 16)` represents the embeddings of 4 sentences, each with a sequence length of 6 tokens, where each token is represented by a 16-dimensional embedding vector."
   ],
   "id": "f0d4dcce1b1894b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:20.597198Z",
     "start_time": "2024-07-24T19:11:20.247593Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings.numpy()[0]",
   "id": "a1d748b9d2a67009",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43membeddings\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding\n",
    "\n",
    "The main purpose of positional encodings is to give the model some information about the order of words or the relative positions of words within a sequence. This is crucial for tasks involving language where the meaning of a sentence can change dramatically based on the order of words (e.g., \"I like dogs more than cats\" vs \"I like cats more than dogs\").\n",
    "\n"
   ],
   "id": "78927f1900b3bf01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:50.076377Z",
     "start_time": "2024-07-24T19:11:50.069721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, max_len:int, dropout:float,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding layer.\n",
    "\n",
    "        :param d_model: The size of each embedding vector.\n",
    "        :param max_len: The maximum number of positions for which embeddings will be created.\n",
    "        :param droupout: The dropout rate to apply to the output of this layer.\n",
    "        :param kwargs: pass\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout)\n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "        \n",
    "    def _get_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Generates the positional encodings using sinusoidal patterns.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: A tensor containing positional encodings of shape (1, max_len, d_model).\n",
    "        \"\"\"\n",
    "        positions = np.arange(self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))  # Shape (d_model/2,)\n",
    "        \n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        pe[:, 0::2] = np.sin(positions * div_term)\n",
    "        pe[:, 1::2] = np.cos(positions * div_term)\n",
    "        \n",
    "        pe = pe[np.newaxis, ...]  # Add a new batch dimension (1, max_len, d_model)\n",
    "        return tf.cast(pe, tf.float32)\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        assert seq_len <= self.max_len, \"Input sequence length exceeds the maximum length\"\n",
    "        \n",
    "        x = x + self.positional_encoding[:, :seq_len]\n",
    "        return self.dropout(x)\n",
    "    \n"
   ],
   "id": "da6da8613c516afc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:52.840627Z",
     "start_time": "2024-07-24T19:11:52.818589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters for the model\n",
    "d_model = 64  # Dimensionality of the embeddings\n",
    "max_len = 10  # Assume max sentence length\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the PositionalEncoding layer\n",
    "pos_encoding_layer = PositionalEncoding(d_model, max_len, dropout_rate)\n",
    "\n",
    "# Generate dummy embeddings for 4 sentences, each with varying lengths\n",
    "dummy_embeddings = tf.random.normal((4, max_len, d_model))  # Shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "# Apply the PositionalEncoding layer\n",
    "encoded_embeddings = pos_encoding_layer(dummy_embeddings)\n",
    "\n",
    "# Print the shape and some values to verify\n",
    "print(\"Shape of encoded embeddings:\", encoded_embeddings.shape)\n",
    "print(\"Sample values from encoded embeddings:\", encoded_embeddings[0, :3, :5])  # Print first 3 positions of the first sentence\n"
   ],
   "id": "3c74fbc77dfb8fb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded embeddings: (4, 10, 64)\n",
      "Sample values from encoded embeddings: tf.Tensor(\n",
      "[[ 0.2522655  -0.23548222 -0.00670897  0.5449405   0.19384186]\n",
      " [ 0.85453874  1.8468276   1.180588   -1.2417519   0.95052147]\n",
      " [ 1.1902826  -0.95571125  0.45968604  1.9030703  -0.71046615]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Add & Norm Layer\n",
    "\n",
    "1. **Residual Connection (Add)**: This step adds the original input of the sub-layer to the output of the sub-layer (like self-attention or feed-forward network), creating a shortcut connection.\n",
    "2. **Layer Normalization (Norm)**: This step normalizes the sum to stabilize and accelerate training by normalizing the output across the features.\n"
   ],
   "id": "85f0b616b55eb00d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:11:58.621378Z",
     "start_time": "2024-07-24T19:11:58.618308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AddAndNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the AddAndNorm layer.\n",
    "\n",
    "        :param epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        :param kwargs: Additional arguments for the layer.\n",
    "        \"\"\"\n",
    "        super(AddAndNorm, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "        \n",
    "    def call(self, x, sub_layer_output):\n",
    "        return self.layer_norm(x + sub_layer_output)\n",
    "        "
   ],
   "id": "931745d4c44ec82d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:12:00.459741Z",
     "start_time": "2024-07-24T19:12:00.433987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = tf.random.uniform((4, 6, 16))  # Example input tensor\n",
    "sub_layer_output = tf.random.uniform((4, 6, 16))  # Example sub-layer output\n",
    "\n",
    "add_and_norm_layer = AddAndNorm()\n",
    "output = add_and_norm_layer(x, sub_layer_output)\n",
    "print(output)\n"
   ],
   "id": "e02e9d32abb7be4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.4496566  -1.0524856   1.7421949  -0.09697199  0.28052068\n",
      "   -1.5112764  -0.95312357 -1.5333147   2.144165    0.55519867\n",
      "    0.6115465   0.07243443  0.4905715   0.4296434  -0.51799023\n",
      "   -0.21145415]\n",
      "  [-1.7803743   0.29167497  1.0718642  -0.54754376 -1.153799\n",
      "   -1.0177886   0.3334664   1.5577394   1.7551833  -0.31663418\n",
      "    0.19967425 -0.9741469  -0.8756956   0.12083232  0.1440965\n",
      "    1.1914519 ]\n",
      "  [ 0.5983474   0.66451335  0.30476356 -1.5932076   0.14489317\n",
      "   -2.070825    0.1133697  -0.16246605  0.73822093  0.10525012\n",
      "    0.4852109  -2.1929255   0.17858481  0.6603012   0.7894263\n",
      "    1.2365437 ]\n",
      "  [ 0.16917276 -0.54689157  0.86132574 -0.06252742 -0.6400471\n",
      "   -0.69738543  0.4390216  -1.1846021  -0.2146728  -0.18351555\n",
      "    0.2744403   2.0240514  -1.9318924  -0.9614123   1.3562579\n",
      "    1.2986755 ]\n",
      "  [-0.85089135  0.01155877  0.60607076 -1.4382744   0.40612197\n",
      "   -0.7820573  -0.03805399 -1.014247   -0.8299135  -1.2430975\n",
      "    0.88877225  2.5609734  -0.18020558  0.35554266  0.9905596\n",
      "    0.5571439 ]\n",
      "  [ 1.1708508  -0.9011537   0.43799567 -0.250939   -0.39131045\n",
      "   -0.6385875   0.86041856  1.2412169  -1.0649698  -1.4071281\n",
      "    0.73434687  0.26814914  1.2324762   1.3726385  -1.4792342\n",
      "   -1.1847742 ]]\n",
      "\n",
      " [[ 0.22869515  1.2558758   1.1354163  -1.2293828   0.42498827\n",
      "    1.254703    0.8781254  -1.6184311  -0.84878254  0.4735384\n",
      "   -0.32157254 -0.43937492 -0.35354924 -1.4276623   1.4869483\n",
      "   -0.899534  ]\n",
      "  [ 0.6124425   0.47139287  0.14320254 -1.3609234   2.126453\n",
      "    1.3012643  -2.4554453   0.18300867 -0.61538863 -0.06167436\n",
      "   -0.49742842  0.5746417   0.25859594 -0.14527512 -0.44205093\n",
      "   -0.09281754]\n",
      "  [ 0.6708729   0.69966626 -1.891646    0.2500813  -1.4833398\n",
      "    0.5099499   0.43485522 -0.87095284 -1.2052453   0.9165499\n",
      "   -1.2071195   0.42106152  1.8396757   0.62227917  0.5138273\n",
      "   -0.2205193 ]\n",
      "  [-0.94968426 -1.7754351   0.29390025  1.7346001  -0.6478822\n",
      "   -1.1729717  -0.15680194  0.31079817  0.2165389   1.2188718\n",
      "    0.8378968   0.3988719   0.03459835 -1.8290348   0.5308838\n",
      "    0.95485115]\n",
      "  [-0.30277395 -0.825703   -0.5423448  -0.50427365  0.7947428\n",
      "    0.9028857   0.9252682  -0.46910405  0.6075287   0.47876525\n",
      "    0.53504634 -1.9336984  -1.0471935   1.1105249  -1.4718133\n",
      "    1.7421463 ]\n",
      "  [-0.45288134  0.25380135 -1.874673    0.42934704  1.3570209\n",
      "    0.7959223  -1.0523849   0.9929812  -1.0133002  -0.76198936\n",
      "   -1.2599367   1.0277157   0.1980288  -0.35362196 -0.03154135\n",
      "    1.7455127 ]]\n",
      "\n",
      " [[ 0.19719338 -1.1906848   0.22577012 -0.48398685  0.5585141\n",
      "    1.4475284  -1.556891   -1.5298142   0.77772355 -0.6822847\n",
      "   -0.2640201   1.7145553  -0.22104931  1.6537821  -0.4069823\n",
      "   -0.23935413]\n",
      "  [-0.8115808  -0.3427105  -0.2648492   0.89894056  1.3763525\n",
      "    1.4088564   1.2649062   0.13648272 -0.54045177 -1.2988356\n",
      "   -0.14449668 -1.927633   -0.1085856   1.5481753  -0.52710176\n",
      "   -0.6674701 ]\n",
      "  [ 0.2153902  -1.2126195   0.5116024  -1.7717092   0.18652272\n",
      "   -1.2164367   1.1670206   0.5499277  -0.2530644   1.4127381\n",
      "   -1.9573795   0.47401595  0.13892317  1.2260373   0.5146942\n",
      "    0.01433897]\n",
      "  [-0.6170175   0.48430276  0.6493051  -0.5527568   0.46815705\n",
      "    0.13431692 -0.80103004  0.54555845  2.0580692  -0.42984045\n",
      "   -1.1402059  -0.6417341  -0.8305956   0.11979294  2.078395\n",
      "   -1.5247173 ]\n",
      "  [ 1.3646495  -0.5792608  -0.64570904  0.601526    1.1634014\n",
      "   -2.4996715  -1.2093816  -0.21136594  0.6081612   1.4211328\n",
      "    0.0385704  -0.75431895  0.17067742  0.8804109   0.03655839\n",
      "   -0.3853793 ]\n",
      "  [ 1.0304005  -0.15136027  1.0473869  -0.65568364  0.47179604\n",
      "   -0.20932198 -0.51566553  0.3053875  -1.5667515  -2.142446\n",
      "   -0.7817825   1.0276754  -0.82321393  1.5319843   0.7409575\n",
      "    0.69063854]]\n",
      "\n",
      " [[-1.0567455   0.28314495 -0.6123903   2.316073   -2.189458\n",
      "    0.27456236  0.00865149  0.1472671   0.546865    1.5041199\n",
      "   -0.16303444  0.4609716  -0.15575123 -1.0300695   0.17292833\n",
      "   -0.5071347 ]\n",
      "  [-0.5043199  -1.2473345  -1.389713   -1.5934725  -0.5749779\n",
      "    1.611352    0.69400454  0.38470674  0.9881718   2.0000043\n",
      "    0.28219652 -0.7101481  -0.1737976   0.22733283  0.51298046\n",
      "   -0.50698805]\n",
      "  [ 1.0752523   0.03054428 -0.5302191   0.9914601   0.45551634\n",
      "    1.2493131  -0.26803112 -0.7618716  -1.3483697  -1.0352199\n",
      "   -0.09268165 -1.7886983   1.6635096   1.2866948  -0.78576124\n",
      "   -0.14143634]\n",
      "  [-1.3254402   0.2814114  -2.0217192   0.43764567  1.5313962\n",
      "    0.34325218  0.772841    1.526346    0.39637637 -0.7817749\n",
      "    0.97306967 -0.7232139  -1.423074   -0.24507761 -0.13406396\n",
      "    0.39202285]\n",
      "  [-0.6155157   0.08072376 -0.919901   -1.098355    0.63415337\n",
      "    0.34021425 -1.6565654   0.9608576  -0.3789202  -0.15433204\n",
      "   -0.30177844  0.38328695  0.4206879   2.7038794  -1.0370377\n",
      "    0.638602  ]\n",
      "  [-1.2616731  -0.3135128  -2.2557387   0.42461157 -0.38896608\n",
      "    0.9382851   1.1395094   0.56169033 -0.73910904 -1.4423773\n",
      "    0.7995603   0.34649158 -0.08585191  1.0913322   1.3371747\n",
      "   -0.15142441]]], shape=(4, 6, 16), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "The \"Feed Forward\" layer in transformer architectures refers to a fully connected feed-forward network applied to each position separately and identically. Typically, it consists of two linear transformations with a ReLU activation in between. The purpose of this layer is to introduce non-linearity and expand the dimensionality of the embeddings before reducing it back to the original size.\n",
    "\n"
   ],
   "id": "f16ae257e1d49c98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:30:04.277102Z",
     "start_time": "2024-07-24T19:30:04.272750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout:float, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward layer.\n",
    "\n",
    "        :param d_model: The size of the input and output embeddings.\n",
    "        :param d_ff: The hidden layer size of the feed-forward network.\n",
    "        :param dropout_rate: The dropout rate to apply to the output of the feed-forward network.\n",
    "        \"\"\"    \n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the FeedForward layer.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor after applying the feed-forward network and dropout.\n",
    "        \"\"\"\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    "
   ],
   "id": "9034d853d1e841f7",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:30:06.436548Z",
     "start_time": "2024-07-24T19:30:06.429311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [\n",
    "    \"The quick brown fox jumps.\",\n",
    "    \"Over the lazy dog.\",\n",
    "    \"And runs away swiftly.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the sequences to the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "\n",
    "# Define the embedding size (d_model)\n",
    "d_model = 16\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "\n",
    "embeddings_output = embedding_layer(padded_sequences)\n",
    "embeddings_output.shape"
   ],
   "id": "a316dc7985acd242",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:30:08.724170Z",
     "start_time": "2024-07-24T19:30:08.664438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_ff = 64\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create an instance of the FeedForward layer\n",
    "feed_forward_layer = FeedForward(d_model, d_ff, dropout_rate)\n",
    "\n",
    "# Apply the FeedForward layer to the embeddings\n",
    "feed_forward_output = feed_forward_layer(embeddings_output)\n",
    "\n",
    "print(feed_forward_output)\n"
   ],
   "id": "c4af9b3e1a861a9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.71188936e-02  6.78350870e-03  2.29200274e-02  2.68865805e-02\n",
      "   -4.55330499e-03  1.41850803e-02  1.14909336e-02 -1.29317483e-02\n",
      "    5.06777782e-03 -2.74961591e-02  2.09067333e-02 -1.19833965e-02\n",
      "   -4.70610248e-04  9.44923796e-03  4.88287024e-03 -8.06903001e-03]\n",
      "  [-9.69667733e-03 -3.57569050e-04  6.39089104e-03 -5.05737960e-03\n",
      "   -1.28823724e-02 -3.19876634e-02 -4.64992924e-03 -7.85883516e-03\n",
      "   -4.62983875e-03 -4.94833989e-03  2.39656046e-02  1.49299838e-02\n",
      "    1.50686931e-02  4.43587219e-03  2.12711990e-02  2.38423399e-03]\n",
      "  [ 1.76993553e-02  3.86869013e-02  4.39423062e-02 -3.87991453e-03\n",
      "   -1.29083218e-02 -5.95324440e-03 -2.10746266e-02  1.75266769e-02\n",
      "    4.11043502e-03 -3.22539620e-02 -4.36760299e-03 -1.20009920e-02\n",
      "   -1.62505582e-02 -2.92878803e-02  4.82356735e-02  1.41240349e-02]\n",
      "  [ 1.07932063e-02 -3.91062873e-04 -3.00994376e-03  1.04154097e-02\n",
      "   -6.57092081e-03 -2.99333548e-03 -1.41826756e-02 -4.08469839e-03\n",
      "   -2.22979533e-03  2.03259056e-03  2.47755880e-03 -6.88028941e-03\n",
      "   -3.52948415e-03  4.13296325e-03  2.81912517e-02  1.35750677e-02]\n",
      "  [ 2.65341736e-02  2.09920611e-02  1.26338834e-02 -3.27371084e-03\n",
      "   -2.38427557e-02 -1.39776515e-02 -2.78055426e-02 -1.64314844e-02\n",
      "   -1.06527554e-02 -2.28212662e-02 -9.90005210e-03 -1.73744857e-02\n",
      "    1.85148381e-02 -1.80429723e-02  5.06511964e-02  5.36251767e-03]]\n",
      "\n",
      " [[ 3.04122102e-02  1.42751653e-02  5.52256545e-03  1.82634536e-02\n",
      "   -1.47014093e-02 -1.84339415e-02 -8.09494685e-03  1.00210626e-02\n",
      "   -2.07968652e-02 -3.68447043e-02  1.88057087e-02 -1.35594690e-02\n",
      "    2.33529601e-02 -8.09900742e-03  3.05575337e-02 -5.81102725e-03]\n",
      "  [-1.71188936e-02  6.78350870e-03  2.29200274e-02  2.68865805e-02\n",
      "   -4.55330499e-03  1.41850803e-02  1.14909336e-02 -1.29317483e-02\n",
      "    5.06777782e-03 -2.74961591e-02  2.09067333e-02 -1.19833965e-02\n",
      "   -4.70610248e-04  9.44923796e-03  4.88287024e-03 -8.06903001e-03]\n",
      "  [ 1.97713729e-02  2.18226332e-02  5.48970960e-02 -5.47776418e-03\n",
      "    1.88297569e-03  4.54433357e-05  2.00510658e-02  1.12594776e-02\n",
      "   -1.75499655e-02 -2.54160147e-02  1.55971982e-02 -1.21081928e-02\n",
      "    2.40778248e-03 -2.41214037e-02  4.25779521e-02 -1.75002415e-03]\n",
      "  [-1.68309584e-02  7.62111461e-03  3.95625085e-03 -4.21524793e-03\n",
      "   -1.41653521e-02 -1.56358145e-02 -1.94975473e-02 -1.86475813e-02\n",
      "   -6.62730634e-03 -1.60194896e-02  1.79023203e-02  2.38438905e-03\n",
      "   -3.03418795e-03 -3.79982917e-03  1.15898196e-02  6.69645553e-04]\n",
      "  [ 2.51687355e-02  3.04669025e-03  3.40462662e-02  7.59464875e-03\n",
      "   -1.13425357e-02  8.64477083e-03  2.33041253e-02 -2.24395003e-03\n",
      "   -2.18936079e-03 -4.26661875e-03  1.08191902e-02 -8.49093776e-03\n",
      "   -7.75338616e-03 -1.07639935e-02  3.39181013e-02  2.38558147e-02]]\n",
      "\n",
      " [[ 2.30365410e-03  2.95231398e-03  3.33140418e-03  3.10949571e-02\n",
      "   -2.76173279e-03 -3.74375912e-03 -1.43688498e-02 -2.29608230e-02\n",
      "    8.05013464e-04 -1.78717095e-02 -1.03492970e-02 -1.58221722e-02\n",
      "   -1.04217287e-02 -6.35760976e-03  6.12377748e-03 -2.77122454e-04]\n",
      "  [ 8.32272321e-03  2.63947845e-02  1.75425736e-03  1.57911256e-02\n",
      "    4.52288566e-03 -3.47272353e-03 -2.55290419e-03  3.15676560e-03\n",
      "   -5.89179341e-03 -6.29928615e-03  5.76188788e-04 -2.42957547e-02\n",
      "   -1.35938292e-02 -4.63433331e-03  1.37269683e-02  1.12874042e-02]\n",
      "  [ 6.57734182e-03 -4.70176432e-03  4.08847537e-03 -1.50187686e-02\n",
      "    3.88721330e-03  2.14035120e-02  1.81625541e-02  5.92490193e-04\n",
      "   -2.30373312e-02 -1.83132403e-02 -4.97659203e-03 -1.74160246e-02\n",
      "   -4.19502752e-03 -1.64036974e-02  1.37158241e-02  2.41988339e-02]\n",
      "  [ 1.47172119e-02  1.49417878e-03  3.10404450e-02 -2.42815237e-03\n",
      "    1.11598875e-02 -9.39390063e-03 -1.81477889e-03 -8.75406712e-03\n",
      "   -1.09099429e-02 -2.05782056e-02  1.36390943e-02 -1.85875166e-02\n",
      "   -7.92797562e-03 -1.11035667e-02 -4.60287929e-03  1.29361805e-02]\n",
      "  [ 2.51687355e-02  3.04669049e-03  3.40462700e-02  7.59464782e-03\n",
      "   -1.13425348e-02  8.64476897e-03  2.33041253e-02 -2.24395283e-03\n",
      "   -2.18936149e-03 -4.26661968e-03  1.08191920e-02 -8.49094056e-03\n",
      "   -7.75339175e-03 -1.07639926e-02  3.39180976e-02  2.38558166e-02]]], shape=(3, 5, 16), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "efd9c536e19874c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:24:17.016764Z",
     "start_time": "2024-07-24T19:24:17.012763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_tensor = tf.random.uniform((4, 6, d_model))  # Shape (batch_size, sequence_length, d_model)\n",
    "input_tensor[0].shape"
   ],
   "id": "9d9d8b9e8d9824f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:35:17.249824Z",
     "start_time": "2024-07-24T19:35:17.240293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = np.random.randn(3, 5, 16)\n",
    "W = np.random.randn(16, 4)\n",
    "\n",
    "tf.matmul(A, W)"
   ],
   "id": "5177f9cf1fdfaf66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5, 4), dtype=float64, numpy=\n",
       "array([[[-1.75939646,  3.40516093, -4.27705087, -1.88455431],\n",
       "        [-3.19374589, -0.87712827, -1.44354896,  1.54755736],\n",
       "        [-1.2952284 , -6.0005745 ,  5.00949191,  4.13150758],\n",
       "        [-2.07921655, -1.49705482, -1.9251128 ,  0.9703732 ],\n",
       "        [-4.73273478, -1.04469464, -4.28149033,  1.69383454]],\n",
       "\n",
       "       [[-2.91213915,  3.2958085 , -2.29178504, -1.39821847],\n",
       "        [-4.73628485, -3.7345663 ,  2.36622727,  4.6875842 ],\n",
       "        [ 2.83105502, -3.72477762, -2.66309375,  0.38037836],\n",
       "        [ 0.57391868,  3.35325835,  2.40535651, -5.84371626],\n",
       "        [ 1.77867598,  4.53325733, -4.18067179, -0.04259554]],\n",
       "\n",
       "       [[-3.56536643, -3.68107326,  1.2038762 , 11.52075844],\n",
       "        [-2.81360129,  1.52812499,  5.39114092, -4.64559375],\n",
       "        [ 3.01634073, -1.23269434,  1.40073692, -2.11019627],\n",
       "        [-0.66859779,  1.85441214, -2.63966033,  3.32437611],\n",
       "        [ 0.3027926 , -1.64211466, -6.31089021,  2.58699243]]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention is a key component of the transformer architecture, enabling the model to focus on different parts of the input sequence to capture contextual relationships. Here is a detailed explanation with the relevant equations and an algorithm in words.\n",
    "\n"
   ],
   "id": "44d0b64701b8c748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads=8, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention layer.\n",
    "\n",
    "        :param d_model: The dimensionality of the input and output embeddings.\n",
    "        :param num_heads: The number of attention heads.\n",
    "        :param kwargs: Additional arguments for the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "\n",
    "        :param x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        :param batch_size: The batch size of the input tensor.\n",
    "        :return: Tensor of shape (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        x = tf.transpose(x, [0, 2, 1, 3, 4])\n",
    "        return x\n",
    "        \n",
    "    def call(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ],
   "id": "c0a66766a4a7855b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:53:57.136761Z",
     "start_time": "2024-07-24T19:53:57.133077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = np.random.randn(3, 2, 4)\n",
    "x\n",
    "\n"
   ],
   "id": "c6bffb2bbc05b932",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.47190042, -0.82041997, -0.3517263 ,  0.61889998],\n",
       "        [ 0.03952871,  0.69015492,  1.07468517,  0.78579719]],\n",
       "\n",
       "       [[ 0.45223838, -0.76107617, -1.08801296, -1.11609575],\n",
       "        [ 0.72553516, -1.53572232,  0.65298136, -1.2677282 ]],\n",
       "\n",
       "       [[-1.93968604, -2.36903857,  0.42170758, -1.07325776],\n",
       "        [ 0.66682809,  1.21227226, -1.36572286,  1.80016498]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T19:55:44.764954Z",
     "start_time": "2024-07-24T19:55:44.761448Z"
    }
   },
   "cell_type": "code",
   "source": "x.reshape(3, -1, 2, 2)",
   "id": "470642703b03ba93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.47190042, -0.82041997],\n",
       "         [-0.3517263 ,  0.61889998]],\n",
       "\n",
       "        [[ 0.03952871,  0.69015492],\n",
       "         [ 1.07468517,  0.78579719]]],\n",
       "\n",
       "\n",
       "       [[[ 0.45223838, -0.76107617],\n",
       "         [-1.08801296, -1.11609575]],\n",
       "\n",
       "        [[ 0.72553516, -1.53572232],\n",
       "         [ 0.65298136, -1.2677282 ]]],\n",
       "\n",
       "\n",
       "       [[[-1.93968604, -2.36903857],\n",
       "         [ 0.42170758, -1.07325776]],\n",
       "\n",
       "        [[ 0.66682809,  1.21227226],\n",
       "         [-1.36572286,  1.80016498]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec24c3be6154a5c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b39a5892bc34e8be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b93a3ed0f12b69d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
